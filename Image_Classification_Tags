from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests
from io import BytesIO
import spacy  # Import spaCy for NLP processing

def load_blip_model():
    """Load a pre-trained BLIP model for image-to-text generation."""
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def preprocess_image(image_path):
    """Load and preprocess an image."""
    if image_path.startswith('http'):
        response = requests.get(image_path)
        image = Image.open(BytesIO(response.content)).convert("RGB")
    else:
        image = Image.open(image_path).convert("RGB")
    return image

def generate_description(processor, model, image):
    """Generate a descriptive caption for the given image."""
    inputs = processor(images=image, return_tensors="pt")
    outputs = model.generate(**inputs)
    caption = processor.decode(outputs[0], skip_special_tokens=True)
    return caption

def generate_tags(description):
    """Generate tags from the description."""
    # Load the spaCy English model
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(description)
    
    # Extract main words (e.g., nouns, proper nouns, and adjectives)
    tags = [token.text for token in doc if token.pos_ in {"NOUN", "PROPN", "ADJ"}]
    return tags

if _name_ == "_main_":
    # Load BLIP model
    processor, model = load_blip_model()

    # Provide an image URL or path
    image_path = "beach.jpg"  # Replace with your image URL or path
    image = preprocess_image(image_path)

    # Generate description
    description = generate_description(processor, model, image)
    print("Generated Description:")
    print(description)

    # Generate tags
    tags = generate_tags(description)
    print("\nGenerated Tags:")
    print("\n#".join(tags))
